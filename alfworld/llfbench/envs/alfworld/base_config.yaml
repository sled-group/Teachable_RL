controller:
  debug: false
  load_receps: true
  type: oracle
dagger:
  action_space: generation
  beam_width: 10
  fraction_assist:
    fraction_assist_anneal_episodes: 50000
    fraction_assist_anneal_from: 1.0
    fraction_assist_anneal_to: 0.01
  fraction_random:
    fraction_random_anneal_episodes: 0
    fraction_random_anneal_from: 0.0
    fraction_random_anneal_to: 0.0
  generate_top_k: 5
  max_target_length: 20
  replay:
    replay_batch_size: 64
    replay_memory_capacity: 500000
    replay_sample_history_length: 4
    replay_sample_update_from: 2
    update_per_k_game_steps: 5
  training:
    max_nb_steps_per_episode: 100
  unstick_by_beam_search: false
dataset:
  data_path: $ALFWORLD_DATA/json_2.1.1/train
  eval_id_data_path: $ALFWORLD_DATA/json_2.1.1/valid_seen
  eval_ood_data_path: $ALFWORLD_DATA/json_2.1.1/valid_unseen
  num_eval_games: -1
  num_train_games: -1
env:
  domain_randomization: false
  expert_timeout_steps: 150
  expert_type: handcoded
  goal_desc_human_anns_prob: 0.0
  hybrid:
    eval_mode: tw
    start_eps: 100000
    thor_prob: 0.5
  regen_game_files: false
  task_types:
  - 1
  - 3
  - 4
  thor:
    save_frames_path: ./videos/
    save_frames_to_disk: false
    screen_height: 300
    screen_width: 300
    smooth_nav: false
  type: AlfredTWEnv
general:
  checkpoint:
    experiment_tag: test
    load_from_tag: not loading anything
    load_pretrained: false
    report_frequency: 1000
  evaluate:
    batch_size: 10
    env:
      type: AlfredTWEnv
    run_eval: true
  hide_init_receptacles: false
  model:
    block_dropout: 0.1
    block_hidden_dim: 64
    decoder_layers: 1
    dropout: 0.1
    encoder_conv_num: 5
    encoder_layers: 1
    n_heads: 1
    recurrent: true
  observation_pool_capacity: 3
  random_seed: 42
  save_path: ./training/
  task: alfred
  training:
    batch_size: 10
    max_episode: 50000
    optimizer:
      clip_grad_norm: 5
      learning_rate: 0.001
    smoothing_eps: 0.1
  training_method: dagger
  use_cuda: true
  visdom: false
logic:
  domain: $ALFWORLD_DATA/logic/alfred.pddl
  grammar: $ALFWORLD_DATA/logic/alfred.twl2
mask_rcnn:
  pretrained_model_path: $ALFWORLD_DATA/detectors/mrcnn.pth
rl:
  action_space: admissible
  beam_width: 10
  epsilon_greedy:
    epsilon_anneal_episodes: 1000
    epsilon_anneal_from: 0.3
    epsilon_anneal_to: 0.1
    noisy_net: false
  generate_top_k: 3
  max_target_length: 20
  replay:
    accumulate_reward_from_final: true
    count_reward_lambda: 0.0
    discount_gamma_count_reward: 0.5
    discount_gamma_game_reward: 0.9
    discount_gamma_novel_object_reward: 0.5
    multi_step: 3
    novel_object_reward_lambda: 0.0
    replay_batch_size: 64
    replay_memory_capacity: 500000
    replay_memory_priority_fraction: 0.5
    replay_sample_history_length: 4
    replay_sample_update_from: 2
    update_per_k_game_steps: 5
  training:
    learn_start_from_this_episode: 0
    max_nb_steps_per_episode: 100
    target_net_update_frequency: 500
vision_dagger:
  maskrcnn_top_k_boxes: 10
  model_type: resnet
  resnet_fc_dim: 64
  sequence_aggregation_method: average
  use_exploration_frame_feats: false
